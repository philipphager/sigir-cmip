defaults:
  - _self_
  - data: mslr10k
  - model: cacm-minus
  - ci: pointwise-ccit
  - env

train_val_trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 10
  accelerator: auto
  devices: 1
  #strategy: single_device # For DDP, pass 'train_val_trainer.strategy=ddp_find_unused_parameters_false' in cmd
  callbacks:
    - _target_: pytorch_lightning.callbacks.EarlyStopping
      monitor: val_loss/dataloader_idx_0
      patience: 1
    - _target_: pytorch_lightning.callbacks.RichProgressBar
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: ${base_dir}checkpoints/
      filename: ${filename}
      monitor: val_loss/dataloader_idx_0
      mode: min
  logger: ???
#  logger: ???

test_trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: -1
  accelerator: auto
  devices: 1
  logger: ???
  callbacks:
    - _target_: pytorch_lightning.callbacks.RichProgressBar

wandb_logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  project: "cm-bias-evaluation"
  entity: "cm-offline-metrics"
  save_dir: wandb
  id: ???

random_state: 0
base_dir: ???
filename: "${hydra:runtime.choices.model}_${hydra:runtime.choices.logging_policy@train_policy}_${hydra:runtime.choices.user_model@train_user}_${hydra:runtime.choices.data}_seed-${random_state}"
