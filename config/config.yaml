defaults:
  - _self_
  - data: mslr10k
  - model: dctr
  - env

train_val_trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 10
  accelerator: auto
  log_every_n_steps: 100
  devices: 1
  #strategy: single_device # For DDP, pass 'train_val_trainer.strategy=ddp_find_unused_parameters_false' in cmd
  callbacks: ???
  logger: ???

test_trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: -1
  accelerator: auto
  devices: 1
  logger: ???
  callbacks:
    - _target_: pytorch_lightning.callbacks.RichProgressBar

early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: Metrics/val/loss/dataloader_idx_0
  patience: 1

progress_bar:
  _target_: pytorch_lightning.callbacks.RichProgressBar

model_checkpoint:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  dirpath: ${base_dir}checkpoints/
  filename: ???
  monitor: Metrics/val/loss/dataloader_idx_0
  mode: min

wandb_logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  project: ??? # Replace with your wandb project name
  entity: ??? # Replace with your wandb entity name
  save_dir: ${base_dir}wandb
  id: ??? # Custom id set by scripts

random_state: 0
base_dir: ???
experiment_name: test-exp
run_name: test-run


train: 0
val: 1
test: 2