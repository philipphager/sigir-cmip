{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cdd0c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from src.data import WandbLoader\n",
    "from src.score import repeated_cross_val_r2_score, get_metric_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4276e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = \"sigir-cmip\"\n",
    "run_name = \"sweep\"\n",
    "\n",
    "loader = WandbLoader(\n",
    "    \"your-entity\",\n",
    "    \"your-project\",\n",
    "    experiment_name,\n",
    "    run_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7cef1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = loader.load_metrics()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e15f98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_data(df, user_model, train_policy, test_policy, drop_na=False):\n",
    "    columns = [\"model\", \"user_model\", \"train_policy\", \"random_state\", \"test/ppl\"]\n",
    "    \n",
    "    ind_df = df[\n",
    "        (df[\"user_model\"] == user_model)\n",
    "        & (df[\"train_policy\"] == train_policy)\n",
    "        & (df[\"test_policy\"] == train_policy)\n",
    "    ][columns]\n",
    "\n",
    "    ind_df = ind_df.rename(columns={\n",
    "        \"test/ppl\": \"in-distribution PPL\"\n",
    "    })\n",
    "    \n",
    "    columns = [\"model\", \"user_model\", \"train_policy\", \"test_policy\", \"random_state\", \"test/nDCG\", \"test/cmi\", \"test/ppl\"]\n",
    "\n",
    "    ood_df = df[\n",
    "        (df[\"user_model\"] == user_model)\n",
    "        & (df[\"train_policy\"] == train_policy)\n",
    "        & (df[\"test_policy\"] == test_policy)\n",
    "    ][columns]\n",
    "\n",
    "    ood_df = ood_df.rename(columns={\n",
    "        \"test/nDCG\": \"nDCG\",\n",
    "        \"test/cmi\": \"CMIP\",\n",
    "        \"test/ppl\": \"out-of-distribution PPL\"\n",
    "    })\n",
    "    \n",
    "    df = ind_df.merge(ood_df, on=[\"model\", \"user_model\", \"train_policy\", \"random_state\"])\n",
    "    \n",
    "    if drop_na:\n",
    "        df = df[df.notna().all(axis=1)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7417870b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_model = \"GradedPBM\"\n",
    "train_policy = \"NoisyOraclePolicy\"\n",
    "test_policy = \"UniformPolicy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c66e33",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chart_df = filter_data(df, user_model, train_policy, test_policy, drop_na=True)\n",
    "chart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a48228",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate fit of Decision Tree Regressor using R2\n",
    "\n",
    "Proportion of variation in `out-of-distribution PPL` that can be explained by combining `[\"in-distribution PPL\", \"nDCG\", \"CMIP\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0618d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "user_models = [\"GradedPBM\", \"GradedDBN\", \"MixtureDBN\", \"GradedCarousel\"]\n",
    "train_policies = [\"NoisyOraclePolicy\", \"LightGBMRanker\"]\n",
    "test_policies = [\"NoisyOraclePolicy\", \"LightGBMRanker\", \"UniformPolicy\"]\n",
    "metric_combinations = [\n",
    "    [\"in-distribution PPL\"],\n",
    "    [\"nDCG\"],\n",
    "    [\"CMIP\"],\n",
    "    [\"in-distribution PPL\", \"nDCG\"],\n",
    "    [\"in-distribution PPL\", \"CMIP\"],\n",
    "    [\"nDCG\", \"CMIP\"],\n",
    "    [\"in-distribution PPL\", \"nDCG\", \"CMIP\"],\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for user_model, train_policy, test_policy in product(*[user_models, train_policies, test_policies]):\n",
    "    if train_policy == test_policy:\n",
    "        continue\n",
    "    \n",
    "    filter_df = filter_data(df, user_model, train_policy, test_policy, drop_na=True)\n",
    "    \n",
    "    if len(filter_df) > 0:    \n",
    "        target = \"out-of-distribution PPL\"\n",
    "        metrics = [\"in-distribution PPL\", \"nDCG\", \"CMIP\"]\n",
    "\n",
    "        for metric_combination in metric_combinations:\n",
    "            X = filter_df[metric_combination].values\n",
    "            y = filter_df[target].values\n",
    "            scores = repeated_cross_val_r2_score(\n",
    "                X,\n",
    "                y,\n",
    "                DecisionTreeRegressor,\n",
    "                n_splits=2,\n",
    "                n_repeats=1000,\n",
    "                use_adjusted_r2=True\n",
    "            )\n",
    "            \n",
    "            for score in scores:  \n",
    "                rows.append({\n",
    "                    \"user_model\": f\"{user_models.index(user_model)}_{user_model}\",\n",
    "                    \"train_policy\": f\"{train_policies.index(train_policy)}_{train_policy}\",\n",
    "                    \"test_policy\": f\"{test_policies.index(test_policy)}_{test_policy}\",\n",
    "                    \"metric_combination\": \", \".join(sorted(metric_combination)),\n",
    "                    \"r2_score\": score,\n",
    "                })\n",
    "                \n",
    "result_df = pd.DataFrame(rows)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c5568",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric_df = result_df.groupby([\"user_model\", \"train_policy\", \"test_policy\", \"metric_combination\"], sort=False)[\"r2_score\"].mean().reset_index()\n",
    "metric_df = metric_df.pivot_table(\n",
    "    index=[\"user_model\", \"train_policy\", \"test_policy\"],\n",
    "    columns=\"metric_combination\",\n",
    "    values=\"r2_score\",\n",
    "    sort=True\n",
    ").reset_index()\n",
    "\n",
    "metric_df = metric_df.round(3)\n",
    "metric_df = metric_df[[\"user_model\", \"train_policy\", \"test_policy\"] + list(result_df.metric_combination.unique())]\n",
    "metric_df.style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16c758",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\" & \".join(list(map(str, metric_df.mean(0).round(3).values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743edbf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6c88c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "alpha = 0.0001\n",
    "target_metric = \"in-distribution PPL, nDCG\"\n",
    "candidate_metrics = ['in-distribution PPL', 'nDCG', 'CMIP', 'in-distribution PPL, nDCG', 'CMIP, in-distribution PPL', 'CMIP, nDCG', 'CMIP, in-distribution PPL, nDCG']\n",
    "rows = []\n",
    "\n",
    "for user_model, train_policy, test_policy in product(*[user_models, train_policies, test_policies]):\n",
    "    source = result_df[\n",
    "        (result_df[\"user_model\"].str.contains(user_model))\n",
    "        & (result_df[\"train_policy\"].str.contains(train_policy))\n",
    "        & (result_df[\"test_policy\"].str.contains(test_policy))\n",
    "    ]\n",
    "    \n",
    "    if len(source) == 0:\n",
    "        continue\n",
    "    \n",
    "    target_source = source[source[\"metric_combination\"] == target_metric]\n",
    "    \n",
    "    for candidate_metric in candidate_metrics:\n",
    "        candidate_source = source[source[\"metric_combination\"] == candidate_metric]\n",
    "        \n",
    "        t, p = ttest_ind(\n",
    "            candidate_source[\"r2_score\"],\n",
    "            target_source[\"r2_score\"],\n",
    "            alternative=\"two-sided\",\n",
    "            equal_var=False\n",
    "        )\n",
    "        \n",
    "        reject_null = p < (alpha / len(candidate_metrics))\n",
    "        \n",
    "        if reject_null and t > 0:\n",
    "            effect = 1 \n",
    "        elif reject_null and t < 0:\n",
    "            effect = -1\n",
    "        else:\n",
    "            effect = 0\n",
    "   \n",
    "        rows.append({\n",
    "            \"user_model\": f\"{user_models.index(user_model)}_{user_model}\",\n",
    "            \"train_policy\": f\"{train_policies.index(train_policy)}_{train_policy}\",\n",
    "            \"test_policy\": f\"{test_policies.index(test_policy)}_{test_policy}\",\n",
    "            \"metric_combination\": candidate_metric,\n",
    "            \"is_significant\": reject_null,\n",
    "            \"p\": p,\n",
    "            \"t\": t,\n",
    "            \"effect\": effect\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec182a76",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(rows)\n",
    "stats_df = stats_df.pivot_table(\n",
    "    index=[\"user_model\", \"train_policy\", \"test_policy\"],\n",
    "    columns=\"metric_combination\",\n",
    "    values=\"effect\",\n",
    "    sort=True\n",
    ").reset_index()\n",
    "stats_df = stats_df[[\"user_model\", \"train_policy\", \"test_policy\"] + list(result_df.metric_combination.unique())]\n",
    "stats_df.style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de5619",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mark(i):\n",
    "    i = float(i)\n",
    "    \n",
    "    if i == 1:\n",
    "        return \"$^\\\\blacktriangle$\"\n",
    "    elif i == -1:\n",
    "        return \"$^\\\\blacktriangledown$\"\n",
    "        \n",
    "    return \"\"\n",
    "\n",
    "metric_columns = list(result_df.metric_combination.unique())\n",
    "\n",
    "for i in range(len(metric_df)):\n",
    "    metric_row = metric_df.iloc[i]\n",
    "    stats_row = stats_df.iloc[i]\n",
    "    \n",
    "    print(\" & \".join([f\"{m:.3f}{get_mark(s)}\" for m, s in list(zip(metric_row[metric_columns].values, stats_row[metric_columns].values))]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
